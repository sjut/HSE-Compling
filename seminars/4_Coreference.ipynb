{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разрешение анафоры и кореференции\n",
    "\n",
    "Традиционный подход к разрешению анафоры/кореференции — классификация пар именных групп (являются кореферентными или нет). В таком случае самая содержательная задача — придумать признаки для классификации. Какие они могут быть:\n",
    "\n",
    "* морфологические (согласование)\n",
    "* синтаксические (роль в предложении, расстояние по дереву, тип вершины и т.д.)\n",
    "* онтологические (один класс, расстояние и т.д.)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В wordnet все организовано в синсеты - наборы синонимов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(wn.all_synsets()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из них существительных:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(wn.all_synsets('n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(wn.all_synsets('v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А прилагательных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(wn.all_synsets('a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синсет для слова можно достать вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ограничить часть речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('content', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У многих из них есть определения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'day'\n",
    "for synset in wn.synsets(word):\n",
    "    print(word + ' - ' + synset.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И даже примеры употребления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'day'\n",
    "for synset in wn.synsets(word):\n",
    "    print(word + ' - ' + ' | '.join(synset.examples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синсеты связаны между собой стандартными отношениями (гипонимии, антонимии, синонимии, меронимии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = wn.synsets('car')[0]\n",
    "s.part_meronyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Антонимы только достаются не очень удобно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('good')[2].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно засунуть это все в граф и нарисовать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "words = wn.synsets('car')\n",
    "hyponyms = words[0].part_meronyms()\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for w in hyponyms: \n",
    "    G.add_edge(words[0].name().split('.')[0],\n",
    "               w.name().split('.')[0], \n",
    "               weight=words[0].wup_similarity(w))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "\n",
    "nx.draw_networkx(G,with_labels=True,node_size=2000,\n",
    "                 font_size=10,\n",
    "                 node_shape='s',\n",
    "                 alpha=0.9,\n",
    "                 node_color='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посчитать близость между синсетами (зависит от того, какой путь нужно пройти по этим связям от одного объекта до другого и есть ли он вообще)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = wn.synsets('car')[0]\n",
    "bike = wn.synsets('bike')[0]\n",
    "\n",
    "bike.path_similarity(car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другие метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lch_similarity(bike, car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.wup_similarity(bike, car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ещё ворднет для русского. С похожим интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wiki_ru_wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_ru_wordnet import WikiWordnet\n",
    "import re\n",
    "ruwn = WikiWordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ruwn.get_synsets('дерево')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in s.get_words():\n",
    "    print(w.lemma())\n",
    "    print(w.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ruwn.get_synsets('дерево')\n",
    "first_synset = words[0]\n",
    "hyponyms = ruwn.get_hyponyms(first_synset)\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for w in hyponyms: \n",
    "    G.add_edge('дерево',\n",
    "               list(w.get_words())[0].lemma())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "\n",
    "nx.draw_networkx(G,with_labels=True,node_size=2000,\n",
    "                 font_size=10,\n",
    "                 node_shape='s',\n",
    "                 alpha=0.9,\n",
    "                 node_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## РуТез\n",
    "[ещё один тезаурус для русского языка](https://www.labinform.ru/pub/ruthes/index.htm)\n",
    "* более 31.5 понятий, 111.5 тысяч различных текстовых входов (слов и выражений русского языка), более 130 тысяч с учетом значений многозначных слов;\n",
    "* раздаётся в XML по запросу, мы будем использовать уже предобработанную версию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/sjut/HSE-Compling/raw/master/seminars/data/relations_with_concepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "rels_list = []\n",
    "with open(\"relations_with_concepts.csv\", newline='', encoding='utf8') as rels:\n",
    "    reader = csv.DictReader(rels, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        rels_list.append(row)\n",
    "\n",
    "\n",
    "def get_supc2(concept_list, rels_list, has_up=True, depth=0, max_depth=-1):\n",
    "    \"\"\"\n",
    "    Get list of all hypernym chains of the query\n",
    "    - up a level\n",
    "    - add all 'выше' concepts to list\n",
    "    [[level_1, level_2.1, level_3.1], [level_1, level_2.2, level_3.2], etc...]\n",
    "\n",
    "    :param concept_list: search input\n",
    "    :param rels_list: imported set of relations\n",
    "    :param max_depth: maximum allowed number of hypernyms\n",
    "    :param has_up: (internal) bool(current top concept has a superconcept)\n",
    "    :param depth: (internal) current depth in the ontology\n",
    "    :return: list of superconcept for every meaning of query\n",
    "    \"\"\"\n",
    "    new_cl = concept_list[:]\n",
    "    if (not has_up) or depth >= max_depth > 0:\n",
    "        return new_cl\n",
    "    has_up = False\n",
    "    for chain in concept_list:\n",
    "        index = new_cl.index(chain)\n",
    "        word = chain[-1]\n",
    "        for row in rels_list:\n",
    "            new_chain = chain[:]\n",
    "            if row['from'].lower() == word.lower() and row['relation'] == 'ВЫШЕ':\n",
    "                new_chain.append(row['to'].lower())\n",
    "                new_cl.insert(index + 1, new_chain)\n",
    "                has_up = True\n",
    "        if has_up:\n",
    "            new_cl.remove(chain)\n",
    "    return get_supc2(new_cl, rels_list, has_up, depth+1, max_depth)\n",
    "\n",
    "\n",
    "def get_supc(concept_list, rels_list, has_up=True, depth=0, max_depth=-1):\n",
    "    \"\"\"\n",
    "    Find list of all hypernyms of query by level down\n",
    "    [[level_1], [level_2.1, level_2.2], [level_3.1, level_3.2, level_3.3], etc...]\n",
    "\n",
    "    :param concept_list: search input\n",
    "    :param rels_list: imported set of relations\n",
    "    :param max_depth: maximum allowed number of hyponyms\n",
    "    :param has_up: (internal) bool(current top concept has a subconcept)\n",
    "    :param depth: (internal) current depth in the ontology\n",
    "    :return: list of subconcepts for every meaning of query\n",
    "    \"\"\"\n",
    "    if (not has_up) or depth >= max_depth > 0:\n",
    "        return concept_list\n",
    "    has_up = False\n",
    "    new_list = []\n",
    "    for word in concept_list[-1]:\n",
    "        for row in rels_list:\n",
    "            if row['from'].lower() == word.lower() and row['relation'] == 'ВЫШЕ':\n",
    "                if all(row['to'].lower() not in hypo for hypo in concept_list):\n",
    "                    new_list.append(row['to'].lower())\n",
    "                    has_up = True\n",
    "    if has_up:\n",
    "        concept_list.append(new_list)\n",
    "    return get_supc(concept_list, rels_list, has_up, depth + 1, max_depth)\n",
    "\n",
    "\n",
    "def get_subc2(concept_list, rels_list, has_down=True, depth=0, max_depth=-1):\n",
    "    \"\"\"\n",
    "    Get list of all hyponym chains for word in query\n",
    "    - down a level\n",
    "    - add all 'ниже' concepts to list\n",
    "    [[level_1, level_2.1, level_3.1], [level_1, level_2.2, level_3.2], etc...]\n",
    "\n",
    "    :param concept_list: search input\n",
    "    :param rels_list: imported set of relations\n",
    "    :param max_depth: maximum allowed number of hyponyms\n",
    "    :param has_down: (internal) bool(current top concept has a subconcept)\n",
    "    :param depth: (internal) current depth in the ontology\n",
    "    :return: list of subconcepts for every meaning of query\n",
    "    \"\"\"\n",
    "    new_cl = concept_list[:]\n",
    "    if (not has_down) or depth >= max_depth > 0:\n",
    "        return new_cl\n",
    "    for chain in concept_list:\n",
    "        has_down = False\n",
    "        index = new_cl.index(chain)\n",
    "        word = chain[-1]\n",
    "        for row in rels_list:\n",
    "            new_chain = chain[:]\n",
    "            if row['from'].lower() == word.lower() and row['relation'] == 'НИЖЕ':\n",
    "                new_chain.append(row['to'].lower())\n",
    "                new_cl.insert(index + 1, new_chain)\n",
    "                has_down = True\n",
    "        if has_down:\n",
    "            new_cl.remove(chain)\n",
    "    return get_subc2(new_cl, rels_list, has_down, depth+1, max_depth)\n",
    "\n",
    "\n",
    "def get_subc(concept_list, rels_list, has_down=True, depth=0, max_depth=-1):\n",
    "    \"\"\"\n",
    "    Find list of all hyponyms of query by level down\n",
    "    [[level_1], [level_2.1, level_2.2], [level_3.1, level_3.2, level_3.3], etc...]\n",
    "\n",
    "    :param concept_list: search input\n",
    "    :param rels_list: imported set of relations\n",
    "    :param max_depth: maximum allowed number of hyponyms\n",
    "    :param has_down: (internal) bool(current top concept has a subconcept)\n",
    "    :param depth: (internal) current depth in the ontology\n",
    "    :return: list of subconcepts for every meaning of query\n",
    "    \"\"\"\n",
    "    if (not has_down) or depth >= max_depth > 0:\n",
    "        return concept_list\n",
    "    has_down = False\n",
    "    new_list = []\n",
    "    for word in concept_list[-1]:\n",
    "        for row in rels_list:\n",
    "            if row['from'].lower() == word.lower() and row['relation'] == 'НИЖЕ':\n",
    "                if all(row['to'].lower() not in hypo for hypo in concept_list):\n",
    "                    new_list.append(row['to'].lower())\n",
    "                    has_down = True\n",
    "    if has_down:\n",
    "        concept_list.append(new_list)\n",
    "    return get_subc(concept_list, rels_list, has_down, depth+1, max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить все гиперонимы для списка концептов (объединенные в списки по уровням)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_supc([['собака'], ['кошка']], rels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить все гиперонимы для списка концептов (объединенные в списки по концепту)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_supc2([['собака'], [\"кошка\"]], rels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить все гипонимы для списка концептов (объединенные в списки по уровням)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subc([['собака']], rels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить все гипонимы для списка концептов (объединенные в списки по концепту)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subc2([['собака']], rels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификатор кореферентности\n",
    "— является ли пара упоминаний (именных групп) кореферентными?\n",
    "Что нужно?\n",
    "* собрать список пар;\n",
    "* представить в виде признаков;\n",
    "* обучить классификатор.\n",
    "\n",
    "Будем использовать данные корпуса [RuCor]() и [код М. Ионова](https://github.com/max-ionov/rucoref) для соревнований Диалога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/max-ionov/rucoref.git\n",
    "#!rm -rf rucoref/external/reference-coreference-scorers\n",
    "#!cd rucoref/external && git clone https://github.com/conll/reference-coreference-scorers.git && cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем размеченные данные - группы (упоминания + кореф-связи) и сам корпус (тексты разобраны Malt-парсером)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/sjut/HSE-Compling/raw/master/seminars/data/Groups.test.txt\n",
    "#!wget https://github.com/sjut/HSE-Compling/raw/master/seminars/data/Groups.train.txt\n",
    "#!wget https://github.com/sjut/HSE-Compling/raw/master/seminars/data/Tokens.parsed.train.txt\n",
    "#!wget https://github.com/sjut/HSE-Compling/raw/master/seminars/data/Tokens.parsed.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 'rucoref'\n",
    "from anaphoralib.corpora import rueval\n",
    "from anaphoralib.tagsets import multeast\n",
    "from anaphoralib.tagsets.utils import same_grammemmes\n",
    "from anaphoralib.experiments import mentionpair\n",
    "from anaphoralib.experiments import coref_utils\n",
    "from anaphoralib import utils\n",
    "from anaphoralib.experiments import utils as exp_utils\n",
    "%cd '..'\n",
    "\n",
    "scorer_path = 'rucoref/external/reference-coreference-scorers/scorer.pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rucoref_train = rueval.RuCorefCorpus(multeast, rueval)\n",
    "rucoref_test = rueval.RuCorefCorpus(multeast, rueval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_utils.load_corpus(rucoref_train, 'Tokens.parsed.train.txt', 'Groups.train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_utils.load_corpus(rucoref_test, 'Tokens.parsed.test.txt', 'Groups.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ok = lambda g: g.tag.startswith('N') or (g.tag.startswith('P') and g.lemma[0] in multeast.coref_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_mentions, gs_group_ids = coref_utils.get_gs_groups(rucoref_test)\n",
    "gs_groups = gs_mentions\n",
    "\n",
    "pred_mentions, pred_group_ids = coref_utils.get_pred_groups(rucoref_test, group_ok)\n",
    "pred_groups = rucoref_test.groups\n",
    "\n",
    "pred_mentions_gold_bound, pred_gold_bounds_ids = coref_utils.get_pred_groups_gold_boundaries(rucoref_test, group_ok)\n",
    "pred_groups_gold_bound = rucoref_test.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_mentions_train, gs_group_ids_train = coref_utils.get_gs_groups(rucoref_train)\n",
    "gs_groups_train = gs_mentions_train\n",
    "\n",
    "pred_mentions_train, pred_group_ids_train = coref_utils.get_pred_groups(rucoref_train, group_ok)\n",
    "pred_groups_train = rucoref_train.groups\n",
    "\n",
    "pred_mentions_gold_bound_train, pred_gold_bounds_ids = coref_utils.get_pred_groups_gold_boundaries(rucoref_train, group_ok)\n",
    "pred_groups_gold_bound_train = rucoref_train.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMentionPairClassifier(mentionpair.MentionPairClassifier):\n",
    "    NEEDS_TRAINING = True\n",
    "    def __init__(self, scorer_path=None):\n",
    "        self.scorer_path = scorer_path\n",
    "    \n",
    "    def train(self, clf, corpus, mentions):\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self.appositives = []\n",
    "        \n",
    "        self.tagset = corpus.tagset\n",
    "        \n",
    "        for i, text in enumerate(corpus.texts):\n",
    "            all_mentions = utils.find_mentions(corpus.groups[i], corpus.tagset)\n",
    "            gs = corpus.gs[i]\n",
    "            words_index = corpus.words_index[i]\n",
    "\n",
    "            \n",
    "            for chain_id in gs['chains']:\n",
    "                chain = gs['chains'][chain_id]\n",
    "                for pair in ((chain[i], chain[i+1]) for i in range(len(chain)-1)):\n",
    "                    text_groups = []\n",
    "                    for pair_elem in pair:\n",
    "                        gs_group = gs['groups'][pair_elem]\n",
    "                        \n",
    "                        words = [text[words_index[shift]] for shift in gs_group['tokens_shifts']]\n",
    "                        head = text[words_index[gs_group['head_shift'][0]]]\n",
    "                        text_groups.append(coref_utils.create_gs_group(gs_group, words, head))\n",
    "                    \n",
    "                    self.data_x.append(self.get_feature_vector(corpus.texts[i], *text_groups))\n",
    "                    self.data_y.append(True)\n",
    "                    \n",
    "                    neg_first = None\n",
    "                    neg_last = None\n",
    "\n",
    "                    for i_mention, mention in enumerate(all_mentions):\n",
    "                        if mention.offset == text_groups[0].offset:\n",
    "                            neg_first = i_mention\n",
    "                        if mention.offset == text_groups[1].offset:\n",
    "                            neg_last = i_mention\n",
    "                        if neg_first and neg_last:\n",
    "                            break\n",
    "                    \n",
    "                    if not neg_first or not neg_last:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_text_groups = all_mentions[neg_first+1:neg_last]\n",
    "                    for neg_pair in ((neg_text_groups[i], neg_text_groups[i+1]) for i in range(len(neg_text_groups)-1)):\n",
    "                        self.data_x.append(self.get_feature_vector(corpus.texts[i], *neg_pair))\n",
    "                        self.data_y.append(False)\n",
    "        \n",
    "        self.clf = clf\n",
    "        self.clf.fit(self.data_x, self.data_y)\n",
    "    \n",
    "    def pair_coreferent(self, pair, groups, words, parse):\n",
    "        vctr = self.get_feature_vector(words, *pair)\n",
    "        return self.clf.predict([vctr])[0]\n",
    "    \n",
    "    def get_feature_vector(self, words, group_1, group_2):\n",
    "        # group_1 — possible antecedent\n",
    "        # group_2 — anaphor\n",
    "        \n",
    "        head_1 = group_1.words[group_1.head] if group_1.type != 'word' else group_1\n",
    "        head_2 = group_2.words[group_2.head] if group_2.type != 'word' else group_2\n",
    "        \n",
    "        is_appo = False\n",
    "        \n",
    "        if not head_1 in words or not head_2 in words:\n",
    "            n_sentences = -1\n",
    "            print('no alignment found')\n",
    "        else:\n",
    "            i = words.index(head_1)\n",
    "            j = words.index(head_2)\n",
    "            \n",
    "            between_groups = words[i+1:j]\n",
    "            n_sentences = sum(1 for gr in between_groups if gr.tag == 'SENT')\n",
    "            \n",
    "            if j - i == 2 and words[i+1].tag.startswith(',') \\\n",
    "                and same_grammemmes('case', (group_1, group_2), self.tagset) \\\n",
    "                and same_grammemmes('number', (group_1, group_2), self.tagset) \\\n",
    "                and same_grammemmes('gender', (group_1, group_2), self.tagset) \\\n",
    "                and group_1.tag.startswith('N') and group_2.tag.startswith('N'):\n",
    "                is_appo = True\n",
    "                self.appositives.append((group_1, group_2, i, j))\n",
    "        \n",
    "        is_demonstrative = lambda w: [tag.startswith('Pd') or w.lemma[i] in {u'этот', u'тот'} for i, tag in enumerate(w.tags)]\n",
    "        demonstr_1 = is_demonstrative(group_1) if len(group_1.lemma) > 1 else [0]\n",
    "        demonstr_2 = is_demonstrative(group_2) if len(group_2.lemma) > 1 else [0]\n",
    "        \n",
    "        filtered_lemma_1 = ' '.join(lemma for (i, lemma) in enumerate(group_1.lemma) if not demonstr_1[i])\n",
    "        filtered_lemma_2 = ' '.join(lemma for (i, lemma) in enumerate(group_2.lemma) if not demonstr_2[i])\n",
    "        \n",
    "        vctr = []\n",
    "        feat_names = []\n",
    "        \n",
    "        pronoun_1 = self.tagset.pos_filters['pronoun'](group_1) and group_ok(group_1)\n",
    "        pronoun_2 = self.tagset.pos_filters['pronoun'](group_2) and group_ok(group_1)\n",
    "        \n",
    "        vctr.append(pronoun_2 and n_sentences == 1)\n",
    "        feat_names.append('dist==1')\n",
    "        \n",
    "        #vctr.append(pronoun_2 and n_sentences == 0)\n",
    "        #feat_names.append('dist==0')\n",
    "        \n",
    "        vctr.append(not pronoun_1 and not pronoun_2 and filtered_lemma_1 == filtered_lemma_2)\n",
    "        feat_names.append('str_match')\n",
    "        \n",
    "        is_animate_1 = self.tagset.extract_feature('animate', group_1) in ('y', 'a')\n",
    "        is_animate_2 = self.tagset.extract_feature('animate', group_2) in ('y', 'a')\n",
    "        sem_class_agreement = (is_animate_1 and is_animate_2) or (not is_animate_1 and not is_animate_2)\n",
    "        \n",
    "        if not pronoun_1:\n",
    "            sem_class_agreement &= group_1.lemma[group_1.head] == group_2.lemma[group_2.head]\n",
    "        \n",
    "        vctr.append(sem_class_agreement)\n",
    "        feat_names.append('sem_class_agreement')\n",
    "        \n",
    "        vctr.append(pronoun_1)\n",
    "        vctr.append(pronoun_2)\n",
    "        feat_names.extend(('i_pronoun', 'j_pronoun'))\n",
    "        \n",
    "        vctr.append(vctr[-1] and vctr[-2])\n",
    "        feat_names.append('both_pronouns')\n",
    "        \n",
    "        vctr.append(self.tagset.extract_feature('number', group_1) == self.tagset.extract_feature('number', group_2))\n",
    "        vctr.append(self.tagset.extract_feature('gender', group_1) == self.tagset.extract_feature('gender', group_2))\n",
    "        feat_names.extend(('number-agr', 'gender-agr'))\n",
    "        \n",
    "        vctr.append(self.tagset.extract_feature('proper', group_1) == 'p' \n",
    "                    and self.tagset.extract_feature('proper', group_2) == 'p')\n",
    "        feat_names.append('both-proper')\n",
    "        vctr.append(any(demonstr_2[:group_2.head+1]))\n",
    "        feat_names.append('anaphor-is-demonstrative')\n",
    "        \n",
    "        vctr.append(is_appo)\n",
    "        feat_names.append('appositive')\n",
    "        \n",
    "        self.feat_names = feat_names\n",
    "        return vctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = MLMentionPairClassifier(scorer_path)\n",
    "clf1.train(DecisionTreeClassifier(random_state=42), rucoref_train, gs_mentions_train)\n",
    "scores, _, _ = clf1.score(rucoref_test, gs_mentions, gs_groups, metrics=('muc', 'bcub', 'ceafm'), heads_only=False)\n",
    "pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MLMentionPairClassifier(scorer_path)\n",
    "clf2.train(LinearSVC(random_state=42), rucoref_train, gs_mentions_train)\n",
    "scores2, _, _ = clf2.score(rucoref_test, gs_mentions, gs_groups, metrics=('muc', 'bcub', 'ceafm'), heads_only=False)\n",
    "pprint(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "Добавьте к морфосинтаксическим фичам что-нибудь, связанное с онтологиями (можно использовать Викисловарь, можно РуТез). Можно скопировать код `MLMentionPairClassifier` и изменить функцию для вычисления вектора фичей.\n",
    "Оцените качество нового классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
