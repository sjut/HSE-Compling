{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import *\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "token = RegexpTokenizer('\\w+')\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize_pm(text):\n",
    "    words = [morph.parse(word)[0].normal_form for word in tokenize(text) if word]\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    return token.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Адаграм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec и многие другие векторные модели сопоставляют 1 вектор. Это значит, что у каждого слова в векторном пространстве только 1 значение. У многозначных слов векторы будут просто каким-то усреднением или обобщением всех его значений. \n",
    "\n",
    "В работе https://arxiv.org/pdf/1502.07257.pdf предлагается способ улучшить Skip Gram, так чтобы каждому слову сопоставлялось K различных векторов, так что каждый из них представляет какое-то из его значений. При этом сам параметр K задавать не нужно, модель сама находит нужное количество \"значений\" для каждого слова.\n",
    "\n",
    "Изначально этот  подход реализован на julia, но есть реализация на питоне - https://github.com/lopuhin/python-adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значения каких-нибудь слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.word_sense_probs('вечер')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие слова близки к каждому из значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на все слова у которых есть хотя бы 2 устойчивых значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous = []\n",
    "for i, word in enumerate(vm.dictionary.id2word):\n",
    "    probs = vm.word_sense_probs(word)\n",
    "    if len(probs) > 1:\n",
    "        ambiguous.append(word)\n",
    "print(ambiguous[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = vm.disambiguate('вечер', normalize(\"Ради любви родителей, ради того, чтобы они снова также танцевали в их гостиной, наслаждаясь милыми семейными\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', np.argmax(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = vm.disambiguate('вечер', normalize(\"абонемент № 19 \\\"Камерные \\\" включает в себя и концерт лауреата последнего Конкурса Чайковского\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.sense_neighbors('вечер', np.argmax(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSD / WSI\n",
    "Разрешение семантической/лексической неоднозначности/омонимии\n",
    "\n",
    "Проверим, насколько хорошо выбирается значение на данных с [соревнования Диалога](http://www.dialog-21.ru/evaluation/2018/disambiguation/) (переиспользую [baseline](https://github.com/nlpub/russe-wsi-kit) соревнования)\n",
    "\n",
    "**NB!** Большая модель AdaGram для русского языка, которую мы используем, обучена на корпусе с нормализацией *mystem*. Так что немного модифицируем нашу функцию нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def disambiguate(model, word, context):\n",
    "    word, _ = lemmatized_context(word)\n",
    "    probs = model.disambiguate(word, lemmatized_context(context))\n",
    "    return 1 + probs.argmax()\n",
    "\n",
    "\n",
    "def lemmatized_context(s):\n",
    "    return [w.lower() for w in mystem.lemmatize(\" \".join(tokenize(s)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.baseline-adagram.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['predict_sense_id'] = [disambiguate(vm, word, context)\n",
    "                          for word, context in tqdm(zip(df['word'], df['context']), total=len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_word = df.groupby('word').apply(lambda f: adjusted_rand_score(f['gold_sense_id'], f['predict_sense_id'])).to_frame('ARI')\n",
    "per_word_ari = per_word['ARI']\n",
    "print('Mean word ARI: %.4f' % np.mean(per_word_ari))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики используется [Adjuster Rand Index](https://en.wikipedia.org/wiki/Rand_index), а [вот ссылка на документацию](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized embeddings\n",
    "[ELMo](https://arxiv.org/pdf/1802.05365.pdf) — модель, которая позволяет получить не просто вектор слова W,\n",
    "а _вектор слова W в контексте C_.\n",
    "Что происходит?\n",
    "Обучаем двунаправленную (bidirectional) языковую модель примерно так*:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/Bert-language-modeling.png\" alt=\"elmo\" width=\"400\"/>\n",
    "\n",
    "Но затем мы не просто берем какие-то представления отдельных слов, а сохраняем все веса и пропускаем каждое \n",
    "предложение для новой задачи через такую сетку с этими весами. Получаем вектора для всех слов в предложении из нескольких слоев!\n",
    "\n",
    "\\* картинка из [блога](https://jalammar.github.io/) чувака по имени Jay Allamar, кстати, очень доступные объяснения всяких NLP-штук с картинками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Немножко кода для загрузки модели\n",
    "class Elmo:\n",
    "    def __init__(self, path=\"\"):\n",
    "        if path:\n",
    "            self.elmo = ElmoEmbedder(options_file=path + \"/options.json\", weight_file=path + \"/model.hdf5\")\n",
    "        else:\n",
    "            self.elmo = ElmoEmbedder()\n",
    "\n",
    "    def get_elmo_vector(self, tokens, layer):\n",
    "        vectors = self.elmo.embed_sentence(tokens)\n",
    "        X = []\n",
    "        for vector in vectors[layer]:\n",
    "            X.append(vector)\n",
    "\n",
    "        X = np.array(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_word_vector(self, word, tokens, layer):\n",
    "        vectors = self.elmo.embed_sentence(tokens)\n",
    "        for v, t in zip(vectors[layer], tokens):\n",
    "            if t == word:\n",
    "                return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Elmo(\"196\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"многочисленные укрепленные монастыри также не являлись замками как таковыми — это были крепости\"\n",
    "tokens = normalize(sentence)\n",
    "v = model.get_elmo_vector(tokens, 0)\n",
    "print(tokens)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_word_vector('замок', tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сначала нарисовать, что получается (пропустим немного заранее заготовленной магии matplotlib и PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction(X, n):\n",
    "    pca = PCA(n_components=n)\n",
    "    print(\"size of X: {}\".format(X.shape))\n",
    "    results = pca.fit_transform(X)\n",
    "    print(\"size of reduced X: {}\".format(results.shape))\n",
    "\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(\"Variance retained ratio of PCA-{}: {}\".format(i+1, ratio))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(word, token_list, labels, reduced_X):\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['ro', 'bo', 'yo', 'go', 'co']\n",
    "    label_color = {}\n",
    "    for i, l in enumerate(set(labels)):\n",
    "        label_color[l] = colors[i]\n",
    "\n",
    "    i = 0\n",
    "    points = []\n",
    "    for j, (tokens, l) in enumerate(zip(token_list, labels)):\n",
    "        color = label_color[l[0]]\n",
    "        for k, w in enumerate(tokens):\n",
    "            if w == word:\n",
    "                ax.plot(reduced_X[i, 0], reduced_X[i, 1], color)\n",
    "                points.append((j, k, reduced_X[i, 0], reduced_X[i, 1]))\n",
    "            i += 1\n",
    "\n",
    "    for p in points:\n",
    "        s = token_list[p[0]]\n",
    "        text = ' '.join(s[min(0, p[1] - 5):min(p[1] + 5, len(s))])\n",
    "\n",
    "        # bold the word of interest in the sentence\n",
    "        text = text.replace(word, r\"$\\bf{\" + word + \"}$\")\n",
    "\n",
    "        plt.annotate(text, xy=p[2:])\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['word'] == 'замок']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = df[df['word']=='замок'][df['gold_sense_id']=='1'].sample(4, random_state=5)\n",
    "sentences_2 = df[df['word']=='замок'][df['gold_sense_id']=='2'].sample(4, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(sentences_1['context']) + list(sentences_2['context'])\n",
    "labels = list(sentences_1['gold_sense_id']) + list(sentences_2['gold_sense_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(\n",
    "    [model.get_elmo_vector(tokens=normalize_pm(sentences[idx]), layer=2) for idx, _ in enumerate(sentences)], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduce = dim_reduction(X=X, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('среда', [normalize_pm(s) for s in sentences],  labels, X_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сделать с этими векторами в целях WSD?\n",
    "* классификатор\n",
    "* кластеризация\n",
    "\n",
    "Попробуем разные методы кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].apply(normalize_pm)\n",
    "    labels = grouped_df.get_group(key)['gold_sense_id'].to_list()\n",
    "    X = []\n",
    "    gold_labels = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        v = model.get_word_vector(key, text, 2)\n",
    "        if v is not None:\n",
    "            X.append(v)\n",
    "            gold_labels.append(labels[i])\n",
    "\n",
    "    if not X:\n",
    "        continue\n",
    "    cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1\n",
    "    \n",
    "    ARI.append(adjusted_rand_score(gold_labels, labels))\n",
    "    \n",
    "    print(key, '  ', adjusted_rand_score(gold_labels, labels))\n",
    "print(np.mean(ARI))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
