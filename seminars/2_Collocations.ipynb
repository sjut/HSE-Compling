{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение коллокаций\n",
    "\n",
    "Коллокации — устойчивые n-граммы (обычно биграммы; есть множество определений, мы пока будем использовать интуитивное представление об устойчивом выражении).\n",
    "\n",
    "Сначала давайте научимся доставать из текста биграммы и заодно фильтровать их по морфологическим характеристикам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    ### YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_ngrams(tokens, n=2, patterns=None):\n",
    "    \"\"\"\n",
    "    Если patterns не None, давайте проверять, что части речи биграммы есть в patterns.\n",
    "    Например, patterns = ['ADJF', 'NOUN']\n",
    "    Подумаем о том, хотим ли мы склеивать два слова из разных предложений.\n",
    "    Можно использовать itertools.islice\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    ### YOUR CODE HERE\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь протестируем на каком-нибудь тексте (давайте считать, что каждая строчка = предложение):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Кречет (лат. Falco rusticolus) — птица из отряда соколообразных семейства соколиных.\n",
    "Самый крупный из соколов. \n",
    "Масса самца чуть больше 1 кг, самки — до 2 кг. \n",
    "Окраска сибирского кречета светлая (светлее лапландских кречетов), но изменчивая: от буровато-серой до почти белой сверху; брюшная сторона беловатая с темным рисунком. \n",
    "Темная полоска у разреза рта («усы») почти незаметна. \n",
    "На надклювье, как у всех соколов, характерный зубец. \n",
    "Лапы жёлтые. \n",
    "Скорость в полёте высокая, после нескольких взмахов птица быстро несётся вперёд, не парит. \n",
    "Сидящий кречет держится прямо.\n",
    "Кречет похож на сапсана, но крупнее и имеет относительно более длинный хвост. \n",
    "Голос также похож на голос сапсана, но грубее и ниже: хриплое «кьяк-кьяк-кьяк» или протяжное «кеек-кеек-кеек». \n",
    "Весной может издавать довольно тихую и высокую трель. \n",
    "Южный горный подвид — алтайский кречет, которого многие специалисты считают подвидом или морфой балобана, — отличается более однообразной темной окраской.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams(normalize(text), n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams(normalize(text), patterns=['ADJF', 'NOUN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмем небольшой корпус, который лежит тут же в папке data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data/ng_1.jsonlines\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем биграммы из первого текста и попробуем просто найти самые частотные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_ngrams(data['content'][0])\n",
    "c = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно не включать энграммы, которые содержат предлоги, союзы и т.д.\n",
    "Попробуем использовать список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')\n",
    "\n",
    "def get_ngrams(tokens, n=2, patterns=None, stoplist=[]):\n",
    "    ngrams = []\n",
    "    ### YOUR CODE HERE\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке есть сочетания, которые попали в список из-за того, что одно слово очень частотное и вообще встречается много в каких контекстах. Нас скорее интересуют случаи, когда слова в большинстве случаев встречаются вместе. Для этого мы можем придумать какие-нибудь формулы, учитывающие частоты слов по отдельности и общую частоту.\n",
    "\n",
    "Самый простой способ - взять частоту энграммы и поделить на сумму количеств упоминаний слов по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, _):\n",
    "    try:\n",
    "        score = bigram_count / ((word_count_a + word_count_b) - bigram_count)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведем функцию, которая будет считать частоты для биграмм и слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(texts, n=2):\n",
    "    word_counter = Counter()\n",
    "    ngram_counter = Counter()\n",
    "    for text in texts:\n",
    "        word_counter.update(text)\n",
    "        ngram_counter.update(get_ngrams(text, 2, stoplist=stop))\n",
    "    \n",
    "    return word_counter, ngram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И функцию, которая считает значение метрики для каждой энграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(word_counter, bigram_counter, scorer, threshold=-100000):\n",
    "    ### YOUR CODE HERE\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    ## если метрика выше порога, добавляем в словарь\n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['content'].apply(normalize).tolist()\n",
    "word_counter, bigram_counter = collect_stats(texts)\n",
    "bigram2score = score_bigrams(word_counter, bigram_counter, scorer_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что пошло не так?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple_smoothed(word_count_a, word_count_b, bigram_count, _, min_count=10):\n",
    "    try:\n",
    "        score = (bigram_count - min_count) / ((word_count_a + word_count_b) - bigram_count)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(word_counter, bigram_counter, scorer_simple_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже приличнее. В [статье](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) про word2vec для склейки устойчивых словосочетаний используют такую штуку (стр. 6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_mwe(word_count_a, word_count_b, bigram_count, len_vocab, min_count=10):\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / (word_count_a * word_count_b)) * len_vocab\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(word_counter, bigram_counter, scorer_mwe)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна популярная метрика - Pointwise Mutual Information (PMI, взаимная информация). \n",
    "\n",
    "$$PMI = \\log{\\frac{p(a,b)}{p(a)p(b)}}$$\n",
    "\n",
    "Для её вычисления используются нормализованные частоты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def scorer_pmi(word_count_a, word_count_b, bigram_count, _, corpus_size, minimum_count=5):\n",
    "    score = (((bigram_count - minimum_count) /corpus_size) / ((word_count_a/corpus_size) * (word_count_b/corpus_size)))\n",
    "    return np.log(score) / -np.log((bigram_count/corpus_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Придется переписать функцию, которая применяет метрику к биграммам, потому что теперь мы хотим учитывать размер корпуса, а не словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(word_counter, bigram_counter, scorer, threshold=-100000):\n",
    "    ### YOUR CODE HERE\n",
    "    bigram2score = Counter()\n",
    "    len_vocab = len(word_counter)\n",
    "    corpus_size = sum(word_counter.values())\n",
    "    for bigram in bigram_counter:\n",
    "        score = scorer(word_counter[bigram[0]], word_counter[bigram[1]], \n",
    "                       bigram_counter[bigram], len_vocab, corpus_size)\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(word_counter, bigram_counter, scorer_pmi)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще метрики для выделения коллокаций — это статистические меры/критерии ассоциации/связи. Популярная в статистике мера — [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) (он же по-русски T-критерий Стьюдента):\n",
    "\n",
    "$$t = \\frac{\\bar{x} - \\mu}{\\sqrt{\\frac{s^2}{n}}}$$\n",
    "\n",
    "где $\\bar{x}$ — наблюдаемое среднее (нормализованная частота биграммы)\n",
    "\n",
    "$\\mu$ — ожидаемое среднее (считаем, что появление каждого слова независимо, то есть произведение вероятностей)\n",
    "\n",
    "$s$ — стандартное отклонение ($s^2$ — дисперсия; \n",
    "выбор слова описывается распределением Бернулли, поэтому $s^2 = p(1-p)$)\n",
    "\n",
    "$n$ — размер выборки (размер корпуса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_ttest(word_count_a, word_count_b, bigram_count, _, corpus_size, minimum_count=5):\n",
    "    mu = ((word_count_a/corpus_size) * (word_count_b/corpus_size))\n",
    "    x_ = (bigram_count/corpus_size)\n",
    "    score = (x_ - mu) / np.sqrt(x_/corpus_size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(word_counter, bigram_counter, scorer_ttest)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ещё много замечательных метрик, и многие из них реализованы в модуле `nltk.collocations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = BigramCollocationFinder.from_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2.nbest(bigram_measures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = finder2.score_ngrams(bigram_measures.dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([x for x in scores if x[1] != 1.0], key=lambda x: x[1], reverse=True)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
